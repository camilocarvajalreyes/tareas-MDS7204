% Template:     Template Reporte LaTeX
% Documento:    Archivo principal
% Versión:      1.2.9 (22/04/2020)
% Codificación: UTF-8
%
% Autor: Pablo Pizarro R.
%        Facultad de Ciencias Físicas y Matemáticas
%        Universidad de Chile
%        pablo@ppizarror.com
%
% Sitio web:    [https://latex.ppizarror.com/reporte]
% Licencia MIT: [https://opensource.org/licenses/MIT]

% CREACIÓN DEL DOCUMENTO
\documentclass[letterpaper,11pt,oneside]{article}

% INFORMACIÓN DEL DOCUMENTO
\def\titulodelreporte {\textbf{Tarea 1 - Filtros en series de tiempo}\\MDS7204 - Aprendizaje de Máquinas Avanzado}
\def\temaatratar {Tarea 1}
\def\fechadelreporte {\today}

\def\autordeldocumento {Camilo Carvajal Reyes}
\def\nombredelcurso {Aprendizaje de Máquinas Avanzado}
\def\codigodelcurso {MDS7204}

\def\nombreuniversidad {Universidad de Chile}
\def\nombrefacultad {Facultad de Ciencias Físicas y Matemáticas}
\def\departamentouniversidad {Departamento de Ingeniería Matemática}
\def\imagendepartamento {departamentos/dim}
\def\localizacionuniversidad {Santiago, Chile}

% CONFIGURACIONES
\input{lib/config}

% IMPORTACIÓN DE LIBRERÍAS
\input{lib/env/imports}

% IMPORTACIÓN DE FUNCIONES Y ENTORNOS
\input{lib/cmd/all}

% IMPORTACIÓN DE ESTILOS
\input{lib/style/all}

% CONFIGURACIÓN INICIAL DEL DOCUMENTO
\input{lib/cfg/init}

% INICIO DE LAS PÁGINAS
\begin{document}
	
% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\input{lib/cfg/page}

% CONFIGURACIONES FINALES
\input{lib/cfg/final}

% ======================= INICIO DEL DOCUMENTO =======================

% Título y nombre del autor
\inserttitle

% Resumen o Abstract
% \begin{abstract}
% 	\lipsum[11]
% \end{abstract}


\section{Introducción}
El presente informe aborda las implementaciones de dos métodos de predicción en series de tiempo: el filtro de Kalman y el filtro de partículas. Ambos integran un conjunto de métodos predictivos secuenciales cuya naturaleza general abordaremos a continuación para el caso unidimensional.

\newp Supongamos que queremos encontrar una sucesión (indexada en los naturales) $X$ definida en los reales. Cada elemento $X_n$ de la sucesión describe algún fenómeno en el instante de tiempo $n$. Diremos que su comportamiento dinámico estará dado por $ X_n = f_n(X_{n-1},V_{n-1})$. Acá cada $f_n$ es una función $\mathbb{R}\times\mathbb{R}\mapsto\mathbb{R}$ que no necesariamente será lineal, y $(V_n)_{n\in\mathhnn{N}}$ son realizaciones independientes de una v.a. que modela ruido. 

\newp Normalmente no tendremos acceso a los valores reales de $X$, pero si podremos hacer observaciones, lo cual nos genera una sucesión $Y$. La naturaleza de tal observaciones estarán dadas por la expresión  $ Y_n = h_n(X_n,W_n)$. Nuevamente los $h_n$ serán funciones $\mathbb{R}\times\mathbb{R}\mapsto\mathbb{R}$ (posiblemente) no lineales y $W$ será una sucesión aleatoria i.i.d. que modela el ruido. % Para llevar a cabo las estimaciones de manera secuencial, tomaremos la probabilidad de ocurrencia de los valores de manera Bayesiana.  % :

\begin{enumerate}
    \item \textbf{Predicción}: Primero computamos la probabilidad del estado siguiente dadas las observaciones hasta el tiempo anterior usando probabilidades totales.
    $$ p(X_n|Y_{1:n-1}) = \displaystyle \int p(X_n|X_{n-1})p(X_{n-1}|Y_{1:n-1})dX_{n-1}$$
    \item \textbf{Actualización}: En este paso incorporamos la información de la medición en el tiempo actual, con lo cual se puede usar el teorema de Bayes.
    $$ p(X_n|Y_{1:n}) = \frac{p(Y_n|X_{n})p(X_n|Y_{1:n-1})}{p(Y_n|Y_{1:n-1})}$$
\end{enumerate}


\section{Filtro de Kalman}

Primero consideremos $f$ como $ f(X_n) = A X_{n-1} + B V_n$ y $h$ como $ h_n(X_n) = C X_n + DW_n$, lo cual corresponde a un modelo lineal, pues $A,B,C,D$ son constantes. Además tanto $V$ como $W$ son sucesiones de ruido Gaussiano $\mathcal{N}(0,1)$. Consideramos estos datos como input para nuestro modelo, que evolucionará en dos etapas:
\begin{enumerate}
    \item \textbf{Predicción}: primeramente se computan las predicciones a priori tanto de $X_n$ como de la varianza, que denotamos $P_n$.
    $$ \hat X_{n|n-1} = A \hat X_{n-1},\hspace{.5cm} P_{n|n-1} = A^2 P_{n-1|n-1} + Q$$
    donde $Q$ es la varianza del ruido en la ecuación de estado. Notemos que en este caso, esto es igual a $B$, pues hemos definido ruidos normales estándar.
    \item \textbf{Actualización}: una vez que la medición se vuelve disponible, podemos incorporarla en nuestra estimación. Esto se hace usando un valor que llamaremos la ganancia de Kalman. Este parámetro nos dice cuanto priorizar la medición versus nuestro estimado usando la confianza en las mediciones, y estará dada por:
    $$ K_n = \frac{P_{n|n-1}}{(P_{n|n-1}+\sigma^2)},\hspace{.5cm} \hat X_{n|n}=\hat X_{n|n-1}+K_n(Y_n-AX_{n|n-1}),\hspace{.5cm} P_{n|n}=(1-K_n)P_{n|n-1}$$
\end{enumerate}


Consideremos un modelo lineal. Sampleamos los valores de las constantes incluyendo el estado inicial $X_0$ obteniendo los siguientes valores y series como muestra la figura \ref{img:series_x_y}:

\begin{table}
\begin{tabular}{|c|c|c|l|l|l|}
\hline
\textbf{parámetro}                   & \textbf{A}                  & \textbf{B}                  & \multicolumn{1}{c|}{\textbf{C}} & \multicolumn{1}{c|}{\textbf{D}} & \multicolumn{1}{c|}{\textbf{Posición inicial}} \\ \hline
\multicolumn{1}{|r|}{\textbf{valor}} & \multicolumn{1}{r|}{0.5488} & \multicolumn{1}{r|}{1.0860} & 0.6027                          & 0.5179                          & 1.4940                                         \\ \hline
\end{tabular}
\end{table}

% \insertimage[\label{img:serie_x}]{img/serie_x}{scale=0.5}{Serie $X$}
% \insertimage[\label{img:serie_y}]{img/serie_y}{scale=0.5}{Serie $Y$}
\insertimage[\label{img:series_x_y}]{img/series_x_y}{scale=0.48}{Serie original $X$ y mediciones $Y$.}

\insertimage[\label{img:x_pred_updt}]{img/x_x_updt_x_pred}{scale=0.48}{Serie X junto con las predicciones del filtro pre y post actualización}
% \insertimage[\label{img:x_updt}]{img/x_x_updt}{scale=0.5}{Serie X junto con las predicciones del filtro}

\insertimagerightline[\label{img:variance_heatmap}]{img/variance_heatmap}{0.52}{15}{MSE versus varianzas del modelo}
La implementación del filtro y su uso sobre la serie $Y$ se grafican en la figura \ref{img:x_pred_updt}. En ella, la serie original $X$ a aproximar se muestra con lineas azules, mientras que los valores que resultan de la aplicación del filtro de Kalman están en verde. Se vislumbra que hay una fuerte correlación entre ambas series. La aproximación se vuelve menos precisa en aquellos puntos de tiempo en los cuales hay saltos repentinos en $X$.

Por otro lado, incluimos también la serie $X_{predicted}$ (en \color{orange}naranjo\color{black}), la cual representa los valores de las predicciones a priori $X_{n:n-1}$ para $n\in\{0,\dots,200\}$. Esto nos permite ver que tan alejada era la aproximación antes de la observación correspondiente. Es claro que la serie luego de la actualización de Kalman (\color{green}verde\color{black}) aproxima de mejor modo $X$ (\color{blue}azul\color{black}), con lo cual podemos confirmar que ponderar usando la ganancia de Kalman efectivamente acerca la predicción final a la serie original. Para poder cuantificar la calidad de las predicciones usamos el error cuadrático medio (MSE), con lo cual obtenemos $0.457$. A modo de referencia, el error de la serie pre-actualización es de $1.329$.

Finalmente, en la figura \ref{img:variance_heatmap} incluimos un mapa de calor que nos ayuda a entender el efecto de las varianzas del problema. Mientras más oscura la celda es mejor pues la predicción está más cerca de la serie original. Esto nos dice que es preferible tener baja varianza en las mediciones, lo cual es intuitivo. Matemáticamente, este valor influye en la confianza que podemos tener de las mediciones (mientras más bajo V, más alta la ganancia de Kalman). Por otro lado, una baja varianza en el proceso nos empeora los resultados. Esto se explica puesto a que tener una peor varianza del medidor que del proceso mismo resultará en mediciones consistentemente malas, con las cuales ningún filtro logrará predicciones adecuadas.

\section{Filtro de Partículas}
Para esta pregunta nos centraremos en un modelo de volatilidad estocástica. Simulamos $500$ pasos de un tal proceso $X$, que se muestran en la figura \ref{img:series_x_y_SV} junto con las mediciones $Y$. Notamos en él la naturaleza multiplicativa del ruido de las mediciones.

\insertimage[\label{img:series_x_y_SV}]{img/series_x_y_SV}{scale=0.45}{Serie original $X$ y mediciones $Y$ para el modelo Volatilidad Estocástica.}

Para llevar a cabo el sampleo de nuestras partículas escogemos la importance distribution siguiente:
$$ q(x_n|x_{n-1}^i,y_n):=p(x_n|x_{n-1}^i) \sim \mathcal{N}(\alpha x^i_{n-1},\sigma^2) $$
Esta distribución cumple la naturaleza Markoviana que buscamos, además de ser una adivinanza razonable de un paso siguiente dado el modelo. Más aún, tal distribución nos facilita el cálculo de los pesos, pues luego sólo basta multiplicar el peso anterior correspondiente por la probabilidad de la nueva observación dada una nueva partícula (previa normalización). Luego de la implementación de un filtro de partículas basado en Sequential Importance Sampling obtenemos el gráfico de la figura \ref{img:particle_SIS}.

\insertimage[\label{img:particle_SIS}]{img/particle_SIS}{scale=0.48}{Serie X junto con las predicciones del filtro sin resampling}

El filtro es poco efectivo para aproximar $X$. Esto se debe a que las trayectorias de las partículas se empiezan a alejar de los valores reales, lo cual causa una inestabilidad que se puede observar desde las primeras $100$ iteraciones. Este resultado es esperable para un filtro basado únicamente en SIS, y su MSE es de $4.539$ para $1000$ partículas (no se observaron cambios significativos con un $N$ más grande). Por otro lado, las aproximaciones mejoran sustancialmente cuando aplicamos resampling, como se vislumbra en la figura \ref{img:particle_SIR}.

\insertimage[\label{img:particle_SIR}]{img/particle_SIR}{scale=0.48}{Serie X junto con las predicciones del filtro con resampling}

Este filtro si muestra un comportamiento estable, ya que muchas de las trayectorias que tienen baja probabilidad de explicar el proceso (dadas las observaciones) serán descartadas en el proceso de resampling. En cada paso obtendremos entonces nuevas partículas que se ajustan de mejor manera a las observaciones.

% FIN DEL DOCUMENTO
\end{document}