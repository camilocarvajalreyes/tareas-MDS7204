% Template:     Template Reporte LaTeX
% Documento:    Archivo principal
% Versión:      1.2.9 (22/04/2020)
% Codificación: UTF-8
%
% Autor: Pablo Pizarro R.
%        Facultad de Ciencias Físicas y Matemáticas
%        Universidad de Chile
%        pablo@ppizarror.com
%
% Sitio web:    [https://latex.ppizarror.com/reporte]
% Licencia MIT: [https://opensource.org/licenses/MIT]

% CREACIÓN DEL DOCUMENTO
\documentclass[letterpaper,11pt,oneside]{article}

% INFORMACIÓN DEL DOCUMENTO
\def\titulodelreporte {\textbf{Tarea 3 - Procesos Gaussianos}}
\def\temaatratar {Tarea 3}
\def\fechadelreporte {\today}

\def\autordeldocumento {Camilo Carvajal Reyes}
\def\nombredelcurso {Aprendizaje de Máquinas Avanzado}
\def\codigodelcurso {MDS7204}

\def\nombreuniversidad {Universidad de Chile}
\def\nombrefacultad {Facultad de Ciencias Físicas y Matemáticas}
\def\departamentouniversidad {Departamento de Ingeniería Matemática}
\def\imagendepartamento {departamentos/dim}
\def\localizacionuniversidad {Santiago, Chile}

% CONFIGURACIONES
\input{lib/config}

% IMPORTACIÓN DE LIBRERÍAS
\input{lib/env/imports}

% IMPORTACIÓN DE FUNCIONES Y ENTORNOS
\input{lib/cmd/all}

% IMPORTACIÓN DE ESTILOS
\input{lib/style/all}

% CONFIGURACIÓN INICIAL DEL DOCUMENTO
\input{lib/cfg/init}

\newtheoremstyle{break}%
    {}{}%
    {\itshape}{}%
    {\bfseries}{}% % Note that final punctuation is omitted.
    {\newline}
    % {\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}
    {\thmname{#1}\thmnote{ #3}}

\theoremstyle{break}
\newtheorem{theorem}{Teorema}[subsection]
\newtheorem{proposition}{Proposición}[subsection]

% INICIO DE LAS PÁGINAS
\begin{document}
	
% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\input{lib/cfg/page}

% CONFIGURACIONES FINALES
\input{lib/cfg/final}

% ======================= INICIO DEL DOCUMENTO =======================

% Título y nombre del autor
\inserttitle

% Resumen o Abstract
En la presenta tarea se presentarán aplicaciones de Procesos Gaussianos usando diferentes Kernels y variando los respectivos parámetros. Se usará la primera serie del conjunto de \href{https://ecg.mit.edu/time-series/}{pulsos sanguineos del MIT}, que parece tener arritmia sinusal respiratoria. Se remueven 30\% de los datos, los cuales se intentan predecir con nuestro proceso ajustado al otro 70\% de los datos. Además se usan las siguientes métricas de evaluación: log-versimilitud negativa (para el conjunto de entrenamiento, i.e., aquella usada para entrenar los hiper-parámetros, y para el conjunto de puntos no-observados) y el error cuadrático medio entre la media y los valores reales (de puntos no observados). Para las métricas en cuestión se usa el posterior computado sobre aquellos puntos de interés.

% Ejemplo, se puede borrar
\section{\textit{Kernel Spectral Mixture}}
Consideramos primeramente el kernel de Mixtura Espectral dado por: 
$$K(x)=\sigma^2 exp(-2\pi^2\gamma^2x^2)cos(2\pi\mu x)\,.$$
Además poseemos un parámetro $\mu_n$ que puede interpretarse como un nivel de confianza en la medición. Para esto, usamos el toolkit \href{https://github.com/GAMES-UChile/The_Art_of_Gaussian_Processes}{gp\_lite}, en la cual este kernel viene implementado. Usar los parámetros por defecto ($\sigma = 10, \gamma = 0.5, \mu = 0.1, \sigma_n = 0.1$) nos da un proceso como el mostrado en la figura \ref{img:untrained} (a). Acá nos movemos en el dominio de los minutos. Cuando la serie la definimos en el dominio de los minutos. Vale señalar que los resultados por defecto varían drásticamente al cambiar el dominio inicial (no así luego de entrenar). En la figura \ref{img:untrained} (b) tenemos una actualización manual de parámetros ($\sigma = 10, \gamma = 0.75, \mu = 0.75, \sigma_n = 0.5$) que muestra un ajuste razonable a nuestra serie original. Esto último mejora, en el conjunto test, de 35102.53 a 26846.84 la log-verosimilitud negativa y de n a 9289.50 el error cuadrático medio.

\begin{images}[\label{img:untrained}]{Posterior para GP sin entrenar}
    \addimage{img/untrained_gp_SM_post}{width=8.2cm}{Parámetros por defecto}
    \addimage{img/SM_params/best_manual}{width=8.2cm}{Parámetros manuales}
\end{images}

\subsection{Análisis de parámetros}
En lo que sigue se realiza un análisis cualitativo de los parámetros del kernel SM. Primero notemos que la variación de $\mu$ se refleja en una frecuencia de oscilación, lo cual se condice con su rol dentro del término coseno. En el gráfico \ref{img:exp-sm} (a) podemos ver el efecto de un valor exageradamente grande, para luego ver un efecto más suave en la parte \ref{img:exp-sm} (b). Por otro lado, las variaciones de el parámetro $sigma$ se manifiestan en menores oscilaciones para parámetros pequeños (no consideramos pertinente incluirlos en el informe). Esto es similar para $\gamma$, en cuyo caso un valor pequeño se manifiesta en un bajo juste a los datos observados (figura \ref{img:exp-sm} (c)). Esto contrasta con el sobre-ajuste que se observa en la figura \ref{img:exp-sm} (d). Esto puesto que el término que multiplica $x$ es demasiado alto y por ende domina el efecto del kernel (con esto no cumple realmente su rol suavizador).

\newp Como parámetro aparte, $\sigma_n$, como es de esperar, es proporcional a la varianza del proceso Gaussiano, con lo cual se admiten posibles más puntos alrededor de la media del posterior. Esto se observa en la sub-figura \ref{img:untrained} (b) anterior.

\begin{images}[\label{img:exp-sm}]{Parámetros variados respecto a kernel por defecto}
    \addimage{img/SM_params/mu_10.0}{width=7.8cm}{$\mu=10.0$}
    \addimage{img/SM_params/mu_1.0}{width=7.8cm}{$\mu=1.0$}
    \newline
    \addimage{img/SM_params/gamma_0.1}{width=8.2cm}{$\gamma=0.1$}
    \addimage{img/SM_params/gamma_1000.0}{width=8.2cm}{$\gamma=1000.0$}
\end{images}

Al minimizar la log-verosimilitud negativa, logramos obtener los parámetros óptimos. Esto se manifiesta en un Proceso Gaussiano ajustado como se visualiza en la figura \ref{img:trained}. En este caso la combinación óptima encontrada es: $\sigma=61.644,\gamma=29.694,\mu=0.012$ y $\sigma_n=0.919$.

\insertimage[\label{img:trained}]{img/trained_gp_SM_post}{scale=0.35}{Posterior para GP entrenada}

\section{Kernel \textit{Radial Basis Function}}

En esta sección abordamos el kernel Función de base radial o RBF por sus siglas en inglés. También llamado exponencial cuadrática ya que está dado por:
$$ K(x) = \sigma^2\exp(-\frac{x^2}{2\gamma^2}) \,.$$
Es el kernel ``por defecto'' en algunas implementaciones, lo cual se debe a sus buenas propiedades. Además, comparte similitudes con el kernel de mixtura espectral, sin embargo su expresión es simplificada. Como primera hipótesis, como para nuestra serie de arritmia funcionaron mejor valores de $\mu$ pequeños, se conjetura que una forma de kernel como la RBF bastaría para modelar el problema, ya que en Spectral Mixture prácticamente se ignoró el término de coseno.

\begin{images}[\label{img:RBFuntrained}]{Posterior para GP con kernel RBF sin entrenar}
    \addimage{img/untrained_gp_RBF_post}{width=8.2cm}{Parámetros por defecto}
    \addimage{img/RBF_params/best_manual}{width=8.2cm}{Parámetros manuales}
\end{images}

Primero notemos que su forma por defecto, que en este caso consideramos como $\sigma=1.0$, $\gamma=1.0$ y $\sigma_n=0.1$, es más suave, como se vislumbra en \ref{img:RBFuntrained} (a). De manera análoga, podemos ``al ojo'' encontrar buenos parámetros, que se ajustan más o menos bien a nuestra serie ($\sigma=5.0$, $\gamma=0.5$ y $\sigma_n=0.25$). Esto ya nos va dando una idea del rol de cada parámetro. Por un lado un $\gamma$ más bien pequeño será como tomar más en cuenta los puntos de la medición. Esto se ve claramente en la figura \ref{img:exp-rbf} (a), donde un valor pequeño de $\gamma$ genera un sobreajuste sobre las observaciones. Por otro lado, el término de escalamiento nos hace acercarnos a la serie original al crecer como se ve en \ref{img:exp-rbf} (b), pese a dejar $\gamma=1$ fijo.

\begin{images}[\label{img:exp-rbf}]{Parámetros variados en Kernel RBF}
    \addimage{img/RBF_params/lscale_0.1}{width=7.8cm}{$\gamma=0.1$}
    \addimage{img/RBF_params/sigma_50.0}{width=7.8cm}{$\sigma=50.0$}
\end{images}

Finalmente, la minimización de la log-verosimilitud negativa nos entrega los parámetros: $\sigma = 57.445$, $\gamma = 0.1569$ y $\sigma_n = 1.033$. La figura \ref{img:trainedRBF} nos sugiere que los resultados son muy similares a lo logrado con el kernel mixtura espectral. Acá ganamos en complejidad, pues tenemos menos términos y por ende una convergencia más rápida a los valores apropiados (la mitad de iteraciones de L-BFGS-B aproximadamente). En cuanto a las métricas usadas, el kernel RBF posee una mejor log-verosimilitud negativa $1987.23$, contra $2036.95$. Sin embargo, la mixtura espectral presenta un error cuadrático medio: $1.25$ versus los $1.52$ de RBF. Ambas métricas computadas sobre el conjunto test.

\insertimage[\label{img:trainedRBF}]{img/trained_gp_RBF_post}{scale=0.35}{Posterior para GP con kernel RBF entrenada}


% FIN DEL DOCUMENTO
\end{document}
